# AI Provider Configuration
# =========================

# Default provider to use (gemini, openai, deepseek)
DEFAULT_PROVIDER=gemini
# Critic routing and strictness
# Choose which provider to use for the critic (gemini|openai|deepseek)
CRITIC_PROVIDER=gemini

# Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_herea
GEMINI_MODEL=gemini-2.5-pro
GEMINI_EMBEDDING_MODEL=gemini-embedding-001
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
GEMINI_EMBEDDING_URL=https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent
GEMINI_CRITIC_MODEL=gemini-2.5-flash-lite
GEMINI_CRITIC_URL=https://generativelanguage.googleapis.com/v1beta

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_EMBEDDING_URL=https://api.openai.com/v1/embeddings
OPENAI_CRITIC_MODEL=gpt-3.5-turbo
OPENAI_CRITIC_URL=https://api.openai.com/v1

# DeepSeek API Configuration
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_EMBEDDING_MODEL=deepseek-embedding
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
DEEPSEEK_EMBEDDING_URL=https://api.deepseek.com/v1/embeddings
DEEPSEEK_CRITIC_MODEL=deepseek-chat
DEEPSEEK_CRITIC_URL=https://api.deepseek.com/v1

# vLLM Configuration (no authentication required), model names are the path, no hf model id.
VLLM_API_KEY=none
VLLM_MODEL=/model-cache/meta-llama--Llama-3.3-70B-Instruct
VLLM_EMBEDDING_MODEL=/model-cache/intfloat--e5-mistral-7b-instruct
VLLM_CRITIC_MODEL=/model-cache/meta-llama--Llama-3.2-3B-Instruct
VLLM_BASE_URL=https://vllm-llama-3-3-70b-namespace.apps.my-domain.com/v1
VLLM_EMBEDDING_URL=https://vllm-e5-mistral-7b-namespace.apps.my-domain.com/v1/embeddings
VLLM_CRITIC_URL=https://vllm-llama-3-2-3b-namespace-apps.my-domain.com/v1

# Model Configuration Matrix
# Available models per provider (comma-separated)
GEMINI_AVAILABLE_MODELS=gemini-2.5-flash,gemini-pro,gemini-pro-vision
OPENAI_AVAILABLE_MODELS=gpt-4,gpt-4-turbo,gpt-3.5-turbo,gpt-4o
DEEPSEEK_AVAILABLE_MODELS=deepseek-chat,deepseek-coder
VLLM_AVAILABLE_MODELS=meta-llama/Llama-3.3-70B-Instruct

# Benchmark/runtime
BENCHMARK_MODE=true
DEFAULT_REQUEST_TIMEOUT=600
MAX_CONCURRENT_REQUESTS=3
MAX_STREAMING_CHUNKS=5000
MAX_RESPONSE_SIZE_MB=500

# Generation defaults
DEFAULT_TEMPERATURE=0.10
DEFAULT_MAX_TOKENS=900000
DEFAULT_TOP_P=0.8
DEFAULT_TOP_K=40

# Critic thresholds
CRITIC_ACCEPT_THRESHOLD=0.55
CRITIC_ACCEPT_THRESHOLD_EASY=0.60
CRITIC_ACCEPT_THRESHOLD_HARD=0.50
REQUIRE_DOUBLE_SUFFICIENT_WHEN_BORDERLINE=false

# Temperature schedule
TEMP_BASE=0.10
TEMP_ADD_STEP=0.15
TEMP_BACKTRACK=0.25
TEMP_CAP=0.90
TEMP_AFTER_AGREEMENT=0.20

# EIG heuristics (optional)
EIG_MIN_DELTA_EASY=0.05
EIG_MIN_DELTA_HARD=0.03
EIG_PLATEAU_ROUNDS_EASY=3
EIG_PLATEAU_ROUNDS_HARD=4

# Benchmark chunking hints (optional)
BENCHMARK_MAX_OUTPUT_TOKENS=900000
BENCHMARK_CRITIC_LIMIT=900000
BENCHMARK_PLAN_LIMIT=500000
BENCHMARK_STEP_LIMIT=800000
BENCHMARK_CODE_LIMIT=900000

# Server Configuration
LOG_LEVEL=DEBUG
ENABLE_CIRCUIT_BREAKER=true
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=60


HF_KEY=ASDF